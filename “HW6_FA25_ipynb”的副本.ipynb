{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**DS5110 - Essentials of Data Science**\n",
        "##**Fall 2025 Homework Assignment 6**\n",
        "\n",
        "Submission Instructions:\n",
        "- Please complete this homework assignment in the same notebook provided.\n",
        "- Submit your completed assignment on Canvas by the deadline.\n",
        "\n",
        "Submission Deadline:\n",
        "**November 18th, 2025**\n",
        "\n",
        "<p align=\"justify\">\n",
        "Please read the instructions carefully when answering questions and ensure your code works correctly before submission. The grader will run your code for grading the coding questions without any adjustment.\n",
        "</p>"
      ],
      "metadata": {
        "id": "rijTPZdl8VyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swUo8ED08Rwf"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Enter your first and last names below:\n",
        "First Name = \"Linxuan\" #@param {type:\"string\"}\n",
        "Last Name = \"Li\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Problem Description**\n",
        "\n",
        "Financial institutions that lend to consumers rely on models to help decide on who to approve or decline for credit (for lending products such as credit cards, automobile loans, or home loans). In this project, your task is to develop models that review credit card applications to determine which ones should be approved. You are given historical data on response (binary default indicator) and 20 predictor variables from credit card accounts for a hypothetical bank XYZ, a regional bank in the Bay area. There are three datasets available: a [training](https://raw.githubusercontent.com/mh2t/DS5110/main/Homework/HW4-Train.csv) dataset with 20,000 accounts; a [validation](https://raw.githubusercontent.com/mh2t/DS5110/main/Homework/HW4-Validation.csv) dataset with 3,000 accounts, and a **hidden** test dataset with 5,000 accounts. Information about the variables is given in the [Appendix](https://github.com/mh2t/DS5110/blob/main/Homework/HW4-appx.pdf).\n",
        "\n",
        "You are asked to do the following and also address specific questions below:\n",
        "\n",
        "* **(10 points)** Do any necessary data pre-processing in preparation for modeling.\n",
        "* **(20 points)** Develop and fit a logistic regression (LR) model, assess its performance, and interpret the results.\n",
        "* **(20 points)** Develop an additional model based on a machine learning (ML) algorithm selected from one of the following: Random Forest, Gradient Boosting (XGBoost or another implementation), or Feedforward Neural Network; assess its performance, and make sure to explain why you chose this particular algorithm.\n",
        "* **(10 points)** Compare the results from the ML algorithm with those from logistic regression model and discuss their advantages and disadvantages; select one of these models for credit approval; and describe the reasons for your selection.\n",
        "* **(5 points)** Describe what performance metrics you chose to evalaute your proposed models and why.\n",
        "* **(10 points)** Describe how you would use it to make decisions on future credit card applications.\n",
        "* **(5 points)** Do customers who already have an account with the financial institution receive any favorable treatment in your model? Support your answer with appropriate analysis.\n",
        "* **(20 points)** 2-page report.\n",
        "* You can use any libraries for this homework.\n",
        "\n"
      ],
      "metadata": {
        "id": "m-775qXD8mti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Deliverables**\n",
        "\n",
        "Please submit the following:\n",
        "\n",
        "1. A report (doc file) that describes all important steps in your data analysis,\n",
        "model development, comparison of the models, and answer to the specific questions in addition to justification for your final model selection. The body of the report should be no more than 2 pages in length (font size 11 and spacing 1.2).\n",
        "2. The codes you used for the analysis should have brief but adequate annotations so that we can run it. Using a format of **IPYNB** is mandatory. Clearly indicate the software packages and versions (if appropriate) that you used for the analysis.\n",
        "3. You are allowed to review textbooks, published papers, websites, and other open literature in preparing for this homework. Note, however, that the material you submit in your report must be based on your own analysis and writing. If you relied on published scholarly work and open-source software for your analysis and findings (beyond what is generally known), you should provide references at the end of the report.\n"
      ],
      "metadata": {
        "id": "aDFDOsITA5RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Top Model Bonus**\n",
        "\n",
        "If the evaluation metric of your chosen model achieve the **highest** rank among all submissions, you will be awarded an additional **10 bonus points**. This bonus will be directly applied to your homework 6 score. It's important to note that the performance of your best model will be assessed using a hidden test set, ensuring a fair and unbiased evaluation."
      ],
      "metadata": {
        "id": "dYibvZqOC3cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "train = pd.read_csv(\"/content/HW4-Train.csv\")\n",
        "valid = pd.read_csv(\"/content/HW4-Validation.csv\")\n",
        "\n",
        "# Example preprocessing\n",
        "train = train.dropna()\n",
        "valid = valid.dropna()\n",
        "\n",
        "y_train = train['Default_ind']\n",
        "X_train = train.drop(['Default_ind'], axis=1)\n",
        "\n",
        "y_valid = valid['Default_ind']\n",
        "X_valid = valid.drop(['Default_ind'], axis=1)\n",
        "\n",
        "X_train_dum = pd.get_dummies(X_train, drop_first=True)\n",
        "X_valid_dum = pd.get_dummies(X_valid, drop_first=True)\n",
        "\n",
        "X_train_dum, X_valid_dum = X_train_dum.align(X_valid_dum, join='left', axis=1, fill_value=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_dum)\n",
        "X_valid_scaled = scaler.transform(X_valid_dum)\n",
        "\n",
        "print(\"X_train_dum shape:\", X_train_dum.shape)\n",
        "print(\"X_valid_dum shape:\", X_valid_dum.shape)\n"
      ],
      "metadata": {
        "id": "RZJTTZDB6NUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6be4e4c-9701-4f2e-a11a-01d8084b537e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_dum shape: (16559, 25)\n",
            "X_valid_dum shape: (2473, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_valid_pred = lr.predict(X_valid_scaled)\n",
        "y_valid_prob = lr.predict_proba(X_valid_scaled)[:, 1]\n",
        "\n",
        "accuracy  = accuracy_score(y_valid, y_valid_pred)\n",
        "precision = precision_score(y_valid, y_valid_pred)\n",
        "recall    = recall_score(y_valid, y_valid_pred)\n",
        "f1        = f1_score(y_valid, y_valid_pred)\n",
        "roc_auc   = roc_auc_score(y_valid, y_valid_prob)\n",
        "cm        = confusion_matrix(y_valid, y_valid_pred)\n",
        "\n",
        "print(\"==== Logistic Regression Performance ====\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n",
        "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWDof5WKLc6E",
        "outputId": "8a46f4ab-bb4d-401b-8baa-8bfa3639595b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Logistic Regression Performance ====\n",
            "Accuracy:  0.9393\n",
            "Precision: 0.7260\n",
            "Recall:    0.2896\n",
            "F1-score:  0.4141\n",
            "ROC-AUC:   0.8213\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2270   20]\n",
            " [ 130   53]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# 1. Define and train Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train_dum, y_train)\n",
        "\n",
        "# 2. Predict on validation set\n",
        "y_valid_pred_rf = rf.predict(X_valid_dum)\n",
        "y_valid_prob_rf = rf.predict_proba(X_valid_dum)[:, 1]\n",
        "\n",
        "# 3. Evaluation metrics\n",
        "acc_rf  = accuracy_score(y_valid, y_valid_pred_rf)\n",
        "pre_rf  = precision_score(y_valid, y_valid_pred_rf)\n",
        "rec_rf  = recall_score(y_valid, y_valid_pred_rf)\n",
        "f1_rf   = f1_score(y_valid, y_valid_pred_rf)\n",
        "auc_rf  = roc_auc_score(y_valid, y_valid_prob_rf)\n",
        "cm_rf   = confusion_matrix(y_valid, y_valid_pred_rf)\n",
        "\n",
        "print(\"==== Random Forest Performance ====\")\n",
        "print(f\"Accuracy:  {acc_rf:.4f}\")\n",
        "print(f\"Precision: {pre_rf:.4f}\")\n",
        "print(f\"Recall:    {rec_rf:.4f}\")\n",
        "print(f\"F1-score:  {f1_rf:.4f}\")\n",
        "print(f\"ROC-AUC:   {auc_rf:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BUiyILAR4MJ",
        "outputId": "73acad76-1cc8-4104-e742-1efaa53443e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Random Forest Performance ====\n",
            "Accuracy:  0.9406\n",
            "Precision: 0.7647\n",
            "Recall:    0.2842\n",
            "F1-score:  0.4143\n",
            "ROC-AUC:   0.8530\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2274   16]\n",
            " [ 131   52]]\n"
          ]
        }
      ]
    }
  ]
}